{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate Calculus\n",
    "Multivariate of multivariable calculus: subtle difference in the number of input and/or output variables. Not much of a problem in calculus and generally of more concern in the field of statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions\n",
    "This process of selecting a candidate function or hypothesis to model a world is what the great geniuses of science are remembered for. There then follows a potentially long and difficult process of testing this hypothesis but there will be nothing to test without that first creative step. Calculus is simply the study of how these functions change with respect to their input variables and it allows you to investigate and manipulate them.\n",
    "\n",
    "$f(x) = x^{2} + 3$ <br>\n",
    "$f(x) = g(x) + h(x-a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derivative\n",
    "Rise increase vertical direction and run increase horizontal direction. <br>\n",
    "$Gradient = \\frac{rise}{run}$ <br>\n",
    "$\\frac{d \\mathbf{f}}{d \\mathbf{x}} = f'(x) = \\lim_{\\Delta x \\to 0} \\frac{f(x + \\Delta x) - f(x)}{\\Delta x}$ <br> <br>\n",
    "__Sum Rule__<br>\n",
    "$\\frac{d}{d \\mathbf{x}}(f(x)+g(x)) = \\frac{d \\mathbf{f(x)}}{d \\mathbf{x}} + \\frac{d \\mathbf{g(x)}}{d \\mathbf{x}}$<br> <br>\n",
    "__Power Rule__ <br>\n",
    "if $f(x) = ax^b$ then $f'(x) = abx^{b-1}$<br><br>\n",
    "__Special Cases__ <br>\n",
    "$f(x) = \\frac{1}{x}$ then $f'(x) = \\frac{-1}{x^2}$<br>\n",
    "$f(x) = e^x$ then $f'(x) = e^x$ <br>\n",
    "Trigonometric functions are exponential functions in disguise: <br>\n",
    "$f(x) = sin(x)$ then $f'(x) = cos(x)$ <br>\n",
    "$f(x) = cos(x)$ then $f'(x) = -sin(x)$ <br> <br>\n",
    "__Product Rule__<br>\n",
    "$if \\space A(x) = f(x)g(x) \\space then \\space \\lim_{\\Delta x \\to 0} \\frac{\\Delta A(x)}{\\Delta x} = \\lim_{\\Delta x \\to 0} f(x)g'(x) + g(x)f'(x)$ <br> <br>\n",
    "__Chain Rule__ <br>\n",
    "if $h=h(p) \\space and \\space p = p(m)$ \n",
    "then $\\frac{d \\mathbf{h}}{d \\mathbf{m}} = \\frac{d \\mathbf{p}}{d \\mathbf{m}} * \\frac{d \\mathbf{h}}{d \\mathbf{p}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables, constants & context\n",
    "__Partial derivative__<br>\n",
    "Independent variables x and dependent variables y. Examples using partial differentiation treating all other variables as constants.<br>\n",
    "$ m =2\\pi r^2 tp + 2 \\pi rh tp$<br><br>\n",
    "$\\frac{\\partial m}{\\partial h} = 2 \\pi rtp$<br><br>\n",
    "$\\frac{\\partial m}{\\partial r} = 4 \\pi tp + 2 \\pi htp$<br><br>\n",
    "$\\frac{\\partial m}{\\partial t} = 2 \\pi r^2 p + 2 \\pi rhp$<br><br>\n",
    "$\\frac{\\partial m}{\\partial p} = 2 \\pi r^2 t  + 2 \\pi rht$<br><br>\n",
    "__Total derivative__<br>\n",
    "$f(x,y,z) = sin(x)e^{yz^2}$<br>\n",
    "$x = t -1; y = t^2; z = \\frac{1}{t}$<br>\n",
    "$\\frac{d \\mathbf{f(x,y,z)}}{d \\mathbf{t}} = \n",
    "\\frac{\\partial f}{\\partial x} \\frac{d \\mathbf{x}}{d \\mathbf{t}}+ \n",
    "\\frac{\\partial f}{\\partial y} \\frac{d \\mathbf{y}}{d \\mathbf{t}}+ \n",
    "\\frac{\\partial f}{\\partial z} \\frac{d \\mathbf{z}}{d \\mathbf{t}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jacobian\n",
    "We now have an algebraic expression for a vector which when we give it a specific  x, y, z coordinate, will return a vector pointing in the __direction of steepest uphil slope__ of this function. Furthermore, the steeper the slope, the greater the magnitude of Jacobian at that point. The Jacobian describes the gradient of a multivariable system. And if you calculate it for a scalar valued multivariable function, you get a row vector pointing up the direction of greater slope, with a length proportional to the local steepness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if $f(x_1, x_2, x_3, ...)$ then $J=[\\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, \\frac{\\partial f}{\\partial x_3}]$ (by convention a row vector instead of column vector) <br> <br>\n",
    "\n",
    "if $u(x,y) = x - 2y$ and $v(x,y) = 3y - 2x$ then <br>\n",
    "$J_u = [\\frac{\\partial u}{\\partial x} \\frac{\\partial u}{\\partial y}]$ <br>\n",
    "$J_v = [\\frac{\\partial v}{\\partial x} \\frac{\\partial v}{\\partial y}]$ <br>\n",
    "\n",
    "\n",
    "Building a Jocabian matrix for vectored valued functions. A matrix transformation from xy space to uv space.<br>\n",
    "$J = \n",
    "\\begin{vmatrix}\n",
    "\\frac{\\partial u}{\\partial x} & \\frac{\\partial u}{\\partial y} \\\\\n",
    "\\frac{\\partial v}{\\partial x}  & \\frac{\\partial v}{\\partial y}\n",
    "\\end{vmatrix}$ <br><br>\n",
    "\n",
    "At J(0,0) = [0,0] then this is probably a maximum, minimum or saddle.\n",
    "\n",
    "However in the real world the function will be much more complicated. Thankfully they may often be considered smooth, meaning that when looking at small or little region of space, they may be considered approximately linear. Therefore, by adding up all the contributions from the Jacobian determinants at each point in space, we can still calculate the change in the size of a region after transformation.\n",
    "\n",
    "In mathematics, __optimisation__ basically means the same thing, as much of the research is dedicated to finding the input values to functions, which correspond to either a maximum or a minimum of a system: global and local maxima as well as minima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hessian\n",
    "For the Jacobian, we collected together all of the first order derivatives of a function into a vector. Now, we're going to collect all of the second order derivatives together into a matrix, which for a function of n variables, would look like this:\n",
    "\n",
    "With f(x,y,z) then \n",
    "$J = \n",
    "\\begin{vmatrix}\n",
    "\\frac{\\partial f}{\\partial x} & \\frac{\\partial f}{\\partial y} & \\frac{\\partial f}{\\partial z}\n",
    "\\end{vmatrix}$ and \n",
    "$H = \n",
    "\\begin{vmatrix}\n",
    "\\partial_{x,x}f & \\partial_{x,y}f & \\partial_{x,z}f \\\\\n",
    "\\partial_{y,x}f & \\partial_{y,y}f & \\partial_{y,z}f \\\\\n",
    "\\partial_{z,x}f & \\partial_{z,y}f & \\partial_{z,z}f \n",
    "\\end{vmatrix}\n",
    "$ \n",
    "\n",
    "It's easier to first calculate the Jacobian and then the Hessian. Hessian matrix is symmetrical along the diagonal.\n",
    "\n",
    "* Det(H) or |H| > 0 means then the point is either minimum or maximum.<br>\n",
    "* if the first term or upper left corner is positive then it is a minimum. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate Chain Rule\n",
    "$f(x_1,x_2,x_3,...,x_n) = f(\\mathbf x)$ and each function of x also a function of t. <br>\n",
    "\n",
    "$\\frac{\\partial f}{\\partial \\mathbf x} = \n",
    "\\begin{vmatrix}\n",
    "\\frac{\\partial f}{\\partial x_1} \\\\\n",
    "\\frac{\\partial f}{\\partial x_2} \\\\\n",
    "\\frac{\\partial f}{\\partial x_3} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial f}{\\partial x_n}\n",
    "\\end{vmatrix} = (J_f)^T \\space\n",
    "$\n",
    "$\\frac{d \\mathbf x}{d t} = \n",
    "\\begin{vmatrix}\n",
    "\\frac{d x_1}{d t} \\\\\n",
    "\\frac{d x_2}{d t} \\\\\n",
    "\\frac{d x_3}{d t} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{d x_4}{d t}\n",
    "\\end{vmatrix} \\space\n",
    "$\n",
    "$\\frac{d f}{d t} = \\frac{\\partial f}{\\partial \\mathbf x} . \\frac{d \\mathbf x}{d t} = J_f \\frac{d \\mathbf x}{d t} $ <br> <br>\n",
    "$\\frac{d f}{d t} = \\frac{\\partial f}{\\partial \\mathbf x} \\frac{\\partial \\mathbf x}{\\partial \\mathbf u} \\frac{d \\mathbf x}{d t} = \n",
    "\\begin{vmatrix}\n",
    "\\frac{\\partial f}{\\partial x_1} & \\frac{\\partial f}{\\partial x_2}\n",
    "\\end{vmatrix} \n",
    "\\begin{vmatrix}\n",
    "\\frac{\\partial x_1}{\\partial u_1} & \\frac{\\partial x_1}{\\partial u_2} \\\\\n",
    "\\frac{\\partial x_2}{\\partial u_1} & \\frac{\\partial x_2}{\\partial u_2}\n",
    "\\end{vmatrix} \n",
    "\\begin{vmatrix}\n",
    "\\frac{d u_1}{d t} \\\\\n",
    "\\frac{d u_2}{d t}\n",
    "\\end{vmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Neural Networks\n",
    "a => 'activity' <br>\n",
    "w => 'weight' <br>\n",
    "b => 'bias' <br>\n",
    "$\\sigma$ => 'activation function' (example tanh or sigmoid functions)\n",
    "\n",
    "$a^{(1)} = \\sigma(w a^{(0)} +b)$\n",
    "\n",
    "Build more complex neural networks by adding more neurons:\n",
    "\n",
    "$a^{(1)} = \\sigma(w_0 a_0^{(0)} + w_1 a_1^{(1)} + b)$<br>\n",
    "$a^{(1)} = \\sigma((\\sum_{j=0}^n w_j a^{(j)}) +b) = \\sigma(\\mathbf w . \\mathbf a^{(0)} + b)$<br>\n",
    "$\\mathbf a^{(1)} = \\sigma(\\mathbf W^{(1)} . \\mathbf a^{(0)} + \\mathbf b^{(1)})$\n",
    "\n",
    "Fully connected forward feed layer with hidden layers:\n",
    "$\\mathbf a^{(L)} = \\sigma(\\mathbf W^{(L)} . \\mathbf a^{(L-1)} + \\mathbf b^{(L)})$\n",
    "\n",
    "Training a neural network is about teaching all the right weights and biases.\n",
    "\n",
    "### Applying Multivariate Chain Rule to train a neural network\n",
    "The classic training method is called back propagation because it looks first at the output neurons and then it works back through the network. However, we can then define a cost function, which is simply the sum of the squares of the differences between the desired output y, and the output that our untrained network currently gives us.\n",
    "\n",
    "$C = \\sum_i (a_{i}^{(L)} - y_i)^2$ <br>\n",
    "$\\mathbf z^{(L)} = \\mathbf W^{(L)} . \\mathbf a^{(L-1)} + \\mathbf b^{(L)}$ <br>\n",
    "$\\mathbf a^{(L)} = \\sigma(\\mathbf z^{(L)})$\n",
    "\n",
    "We could immediately write down a chain rule expression for the partial derivative of the cost with respect to either our weight or our bias: <br>\n",
    "\n",
    "$\\mathbf z^{(1)} = \\mathbf W^{(1)} . \\mathbf a^{(0)} + \\mathbf b^{(1)}$ <br>\n",
    "$\\mathbf a^{(1)} = \\sigma(\\mathbf z^{(1)})$ <br>\n",
    "$C = (a^{(1)} - y)^2$ <br>\n",
    "\n",
    "$\n",
    "\\frac{\\partial C}{\\partial w} = \\frac{\\partial C}{\\partial a^{(1)}} \\frac{\\partial a^{(1)}}{\\partial z^{(1)}} \\frac{\\partial z^{(1)}}{\\partial w}$<br>\n",
    "$\\frac{\\partial C}{\\partial b} = \\frac{\\partial C}{\\partial a^{(1)}} \\frac{\\partial a^{(1)}}{\\partial z^{(1)}} \\frac{\\partial z^{(1)}}{\\partial b}\n",
    "$\n",
    "\n",
    "### Cost Function Training Set\n",
    "$C = \\frac{1}{N} \\sum_k C_k$<br>\n",
    "\n",
    "$\n",
    "\\frac{\\partial C_k}{\\partial w} = \\frac{\\partial C_k}{\\partial a^{(1)}} \\frac{\\partial a^{(1)}}{\\partial z^{(1)}} \\frac{\\partial z^{(1)}}{\\partial w}$<br>\n",
    "$\\frac{\\partial C_k}{\\partial b} = \\frac{\\partial C_k}{\\partial a^{(1)}} \\frac{\\partial a^{(1)}}{\\partial z^{(1)}} \\frac{\\partial z^{(1)}}{\\partial b}\n",
    "$\n",
    "\n",
    "With several output neurons, the individual cost functions remain scalars. Instead of becoming vectors, the components are summed over each output neuron. Note here that \n",
    "*i* labels the output neuron and is summed over, whereas *k* labels the training example. <br>\n",
    "\n",
    "$C_k = \\sum_i (a_i^{(1)} - y_i)^2$ <br>\n",
    "\n",
    "The training data becomes a vector too.\n",
    "\n",
    "$x -> \\mathbf x$ and has the same number of elements as input neurons. <br>\n",
    "$y -> \\mathbf y$ and has the same number of elements as output neurons. <br>\n",
    "\n",
    "This allows us to write the cost function in vector form using the modulus squared.<br>\n",
    "$C_k = |\\mathbf a^{(1)} - \\mathbf y|$ \n",
    "\n",
    "### Backpropagation\n",
    "Training this network is done by back-propagation because we start at the output layer and calculate derivatives backwards towards the input layer with the chain rule. To calculate the derivative of the cost with respect to the weight or bias (generalized any layer):\n",
    "\n",
    "$\n",
    "\\frac{\\partial C_k}{\\partial \\mathbf W^{(i)}} = \\frac{\\partial C_k}{\\partial \\mathbf a^{(N)}} \\frac{\\partial \\mathbf a^{(N)}}{\\partial \\mathbf a^{(N-1)}} ... \\frac{\\partial \\mathbf a^{(i+1)}}{\\partial \\mathbf a^{(i)}} \\frac{\\partial \\mathbf a^{(i)}}{\\partial \\mathbf z^{(i)}} \\frac{\\partial \\mathbf z^{(i)}}{\\partial \\mathbf W^{(i)}}$<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
