{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate Calculus\n",
    "Multivariate of multivariable calculus: subtle difference in the number of input and/or output variables. Not much of a problem in calculus and generally of more concern in the field of statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions\n",
    "This process of selecting a candidate function or hypothesis to model a world is what the great geniuses of science are remembered for. There then follows a potentially long and difficult process of testing this hypothesis but there will be nothing to test without that first creative step. Calculus is simply the study of how these functions change with respect to their input variables and it allows you to investigate and manipulate them.\n",
    "\n",
    "$f(x) = x^{2} + 3$ <br>\n",
    "$f(x) = g(x) + h(x-a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derivative\n",
    "Rise increase vertical direction and run increase horizontal direction. <br>\n",
    "$Gradient = \\frac{rise}{run}$ <br>\n",
    "$\\frac{d \\mathbf{f}}{d \\mathbf{x}} = f'(x) = \\lim_{\\Delta x \\to 0} \\frac{f(x + \\Delta x) - f(x)}{\\Delta x}$ <br> <br>\n",
    "__Sum Rule__<br>\n",
    "$\\frac{d}{d \\mathbf{x}}(f(x)+g(x)) = \\frac{d \\mathbf{f(x)}}{d \\mathbf{x}} + \\frac{d \\mathbf{g(x)}}{d \\mathbf{x}}$<br> <br>\n",
    "__Power Rule__ <br>\n",
    "if $f(x) = ax^b$ then $f'(x) = abx^{b-1}$<br><br>\n",
    "__Special Cases__ <br>\n",
    "$f(x) = \\frac{1}{x}$ then $f'(x) = \\frac{-1}{x^2}$<br>\n",
    "$f(x) = e^x$ then $f'(x) = e^x$ <br>\n",
    "Trigonometric functions are exponential functions in disguise: <br>\n",
    "$f(x) = sin(x)$ then $f'(x) = cos(x)$ <br>\n",
    "$f(x) = cos(x)$ then $f'(x) = -sin(x)$ <br> <br>\n",
    "__Product Rule__<br>\n",
    "$if \\space A(x) = f(x)g(x) \\space then \\space \\lim_{\\Delta x \\to 0} \\frac{\\Delta A(x)}{\\Delta x} = \\lim_{\\Delta x \\to 0} f(x)g'(x) + g(x)f'(x)$ <br> <br>\n",
    "__Chain Rule__ <br>\n",
    "if $h=h(p) \\space and \\space p = p(m)$ \n",
    "then $\\frac{d \\mathbf{h}}{d \\mathbf{m}} = \\frac{d \\mathbf{p}}{d \\mathbf{m}} * \\frac{d \\mathbf{h}}{d \\mathbf{p}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables, constants & context\n",
    "__Partial derivative__<br>\n",
    "Independent variables x and dependent variables y. Examples using partial differentiation treating all other variables as constants.<br>\n",
    "$ m =2\\pi r^2 tp + 2 \\pi rh tp$<br><br>\n",
    "$\\frac{\\partial m}{\\partial h} = 2 \\pi rtp$<br><br>\n",
    "$\\frac{\\partial m}{\\partial r} = 4 \\pi tp + 2 \\pi htp$<br><br>\n",
    "$\\frac{\\partial m}{\\partial t} = 2 \\pi r^2 p + 2 \\pi rhp$<br><br>\n",
    "$\\frac{\\partial m}{\\partial p} = 2 \\pi r^2 t  + 2 \\pi rht$<br><br>\n",
    "__Total derivative__<br>\n",
    "$f(x,y,z) = sin(x)e^{yz^2}$<br>\n",
    "$x = t -1; y = t^2; z = \\frac{1}{t}$<br>\n",
    "$\\frac{d \\mathbf{f(x,y,z)}}{d \\mathbf{t}} = \n",
    "\\frac{\\partial f}{\\partial x} \\frac{d \\mathbf{x}}{d \\mathbf{t}}+ \n",
    "\\frac{\\partial f}{\\partial y} \\frac{d \\mathbf{y}}{d \\mathbf{t}}+ \n",
    "\\frac{\\partial f}{\\partial z} \\frac{d \\mathbf{z}}{d \\mathbf{t}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jacobian\n",
    "We now have an algebraic expression for a vector which when we give it a specific  x, y, z coordinate, will return a vector pointing in the __direction of steepest uphil slope__ of this function. Furthermore, the steeper the slope, the greater the magnitude of Jacobian at that point. The Jacobian describes the gradient of a multivariable system. And if you calculate it for a scalar valued multivariable function, you get a row vector pointing up the direction of greater slope, with a length proportional to the local steepness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if $f(x_1, x_2, x_3, ...)$ then $J=[\\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, \\frac{\\partial f}{\\partial x_3}]$ (by convention a row vector instead of column vector) <br> <br>\n",
    "\n",
    "if $u(x,y) = x - 2y$ and $v(x,y) = 3y - 2x$ then <br>\n",
    "$J_u = [\\frac{\\partial u}{\\partial x} \\frac{\\partial u}{\\partial y}]$ <br>\n",
    "$J_v = [\\frac{\\partial v}{\\partial x} \\frac{\\partial v}{\\partial y}]$ <br>\n",
    "\n",
    "\n",
    "Building a Jocabian matrix for vectored valued functions. A matrix transformation from xy space to uv space.<br>\n",
    "$J = \n",
    "\\begin{vmatrix}\n",
    "\\frac{\\partial u}{\\partial x} & \\frac{\\partial u}{\\partial y} \\\\\n",
    "\\frac{\\partial v}{\\partial x}  & \\frac{\\partial v}{\\partial y}\n",
    "\\end{vmatrix}$ <br><br>\n",
    "\n",
    "At J(0,0) = [0,0] then this is probably a maximum, minimum or saddle.\n",
    "\n",
    "However in the real world the function will be much more complicated. Thankfully they may often be considered smooth, meaning that when looking at small or little region of space, they may be considered approximately linear. Therefore, by adding up all the contributions from the Jacobian determinants at each point in space, we can still calculate the change in the size of a region after transformation.\n",
    "\n",
    "In mathematics, __optimisation__ basically means the same thing, as much of the research is dedicated to finding the input values to functions, which correspond to either a maximum or a minimum of a system: global and local maxima as well as minima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hessian\n",
    "For the Jacobian, we collected together all of the first order derivatives of a function into a vector. Now, we're going to collect all of the second order derivatives together into a matrix, which for a function of n variables, would look like this:\n",
    "\n",
    "With f(x,y,z) then \n",
    "$J = \n",
    "\\begin{vmatrix}\n",
    "\\frac{\\partial f}{\\partial x} & \\frac{\\partial f}{\\partial y} & \\frac{\\partial f}{\\partial z}\n",
    "\\end{vmatrix}$ and \n",
    "$H = \n",
    "\\begin{vmatrix}\n",
    "\\partial_{x,x}f & \\partial_{x,y}f & \\partial_{x,z}f \\\\\n",
    "\\partial_{y,x}f & \\partial_{y,y}f & \\partial_{y,z}f \\\\\n",
    "\\partial_{z,x}f & \\partial_{z,y}f & \\partial_{z,z}f \n",
    "\\end{vmatrix}\n",
    "$ \n",
    "\n",
    "It's easier to first calculate the Jacobian and then the Hessian. Hessian matrix is symmetrical along the diagonal.\n",
    "\n",
    "* Det(H) or |H| > 0 means then the point is either minimum or maximum.<br>\n",
    "* if the first term or upper left corner is positive then it is a minimum. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate Chain Rule\n",
    "$f(x_1,x_2,x_3,...,x_n) = f(\\mathbf x)$ and each function of x also a function of t. <br>\n",
    "\n",
    "$\\frac{\\partial f}{\\partial \\mathbf x} = \n",
    "\\begin{vmatrix}\n",
    "\\frac{\\partial f}{\\partial x_1} \\\\\n",
    "\\frac{\\partial f}{\\partial x_2} \\\\\n",
    "\\frac{\\partial f}{\\partial x_3} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial f}{\\partial x_n}\n",
    "\\end{vmatrix} = (J_f)^T \\space\n",
    "$\n",
    "$\\frac{d \\mathbf x}{d t} = \n",
    "\\begin{vmatrix}\n",
    "\\frac{d x_1}{d t} \\\\\n",
    "\\frac{d x_2}{d t} \\\\\n",
    "\\frac{d x_3}{d t} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{d x_4}{d t}\n",
    "\\end{vmatrix} \\space\n",
    "$\n",
    "$\\frac{d f}{d t} = \\frac{\\partial f}{\\partial \\mathbf x} . \\frac{d \\mathbf x}{d t} = J_f \\frac{d \\mathbf x}{d t} $ <br> <br>\n",
    "$\\frac{d f}{d t} = \\frac{\\partial f}{\\partial \\mathbf x} \\frac{\\partial \\mathbf x}{\\partial \\mathbf u} \\frac{d \\mathbf x}{d t} = \n",
    "\\begin{vmatrix}\n",
    "\\frac{\\partial f}{\\partial x_1} & \\frac{\\partial f}{\\partial x_2}\n",
    "\\end{vmatrix} \n",
    "\\begin{vmatrix}\n",
    "\\frac{\\partial x_1}{\\partial u_1} & \\frac{\\partial x_1}{\\partial u_2} \\\\\n",
    "\\frac{\\partial x_2}{\\partial u_1} & \\frac{\\partial x_2}{\\partial u_2}\n",
    "\\end{vmatrix} \n",
    "\\begin{vmatrix}\n",
    "\\frac{d u_1}{d t} \\\\\n",
    "\\frac{d u_2}{d t}\n",
    "\\end{vmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Neural Networks\n",
    "a => 'activity' <br>\n",
    "w => 'weight' <br>\n",
    "b => 'bias' <br>\n",
    "$\\sigma$ => 'activation function' (example tanh or sigmoid functions)\n",
    "\n",
    "$a^{(1)} = \\sigma(w a^{(0)} +b)$\n",
    "\n",
    "Build more complex neural networks by adding more neurons:\n",
    "\n",
    "$a^{(1)} = \\sigma(w_0 a_0^{(0)} + w_1 a_1^{(1)} + b)$<br>\n",
    "$a^{(1)} = \\sigma((\\sum_{j=0}^n w_j a^{(j)}) +b) = \\sigma(\\mathbf w . \\mathbf a^{(0)} + b)$<br>\n",
    "$\\mathbf a^{(1)} = \\sigma(\\mathbf W^{(1)} . \\mathbf a^{(0)} + \\mathbf b^{(1)})$\n",
    "\n",
    "Fully connected forward feed layer with hidden layers:\n",
    "$\\mathbf a^{(L)} = \\sigma(\\mathbf W^{(L)} . \\mathbf a^{(L-1)} + \\mathbf b^{(L)})$\n",
    "\n",
    "Training a neural network is about teaching all the right weights and biases.\n",
    "\n",
    "### Applying Multivariate Chain Rule to train a neural network\n",
    "The classic training method is called back propagation because it looks first at the output neurons and then it works back through the network. However, we can then define a cost function, which is simply the sum of the squares of the differences between the desired output y, and the output that our untrained network currently gives us.\n",
    "\n",
    "$C = \\sum_i (a_{i}^{(L)} - y_i)^2$ <br>\n",
    "$\\mathbf z^{(L)} = \\mathbf W^{(L)} . \\mathbf a^{(L-1)} + \\mathbf b^{(L)}$ <br>\n",
    "$\\mathbf a^{(L)} = \\sigma(\\mathbf z^{(L)})$\n",
    "\n",
    "We could immediately write down a chain rule expression for the partial derivative of the cost with respect to either our weight or our bias: <br>\n",
    "\n",
    "$\\mathbf z^{(1)} = \\mathbf W^{(1)} . \\mathbf a^{(0)} + \\mathbf b^{(1)}$ <br>\n",
    "$\\mathbf a^{(1)} = \\sigma(\\mathbf z^{(1)})$ <br>\n",
    "$C = (a^{(1)} - y)^2$ <br>\n",
    "\n",
    "$\n",
    "\\frac{\\partial C}{\\partial w} = \\frac{\\partial C}{\\partial a^{(1)}} \\frac{\\partial a^{(1)}}{\\partial z^{(1)}} \\frac{\\partial z^{(1)}}{\\partial w}$<br>\n",
    "$\\frac{\\partial C}{\\partial b} = \\frac{\\partial C}{\\partial a^{(1)}} \\frac{\\partial a^{(1)}}{\\partial z^{(1)}} \\frac{\\partial z^{(1)}}{\\partial b}\n",
    "$\n",
    "\n",
    "### Cost Function Training Set\n",
    "$C = \\frac{1}{N} \\sum_k C_k$<br>\n",
    "\n",
    "$\n",
    "\\frac{\\partial C_k}{\\partial w} = \\frac{\\partial C_k}{\\partial a^{(1)}} \\frac{\\partial a^{(1)}}{\\partial z^{(1)}} \\frac{\\partial z^{(1)}}{\\partial w}$<br>\n",
    "$\\frac{\\partial C_k}{\\partial b} = \\frac{\\partial C_k}{\\partial a^{(1)}} \\frac{\\partial a^{(1)}}{\\partial z^{(1)}} \\frac{\\partial z^{(1)}}{\\partial b}\n",
    "$\n",
    "\n",
    "With several output neurons, the individual cost functions remain scalars. Instead of becoming vectors, the components are summed over each output neuron. Note here that \n",
    "*i* labels the output neuron and is summed over, whereas *k* labels the training example. <br>\n",
    "\n",
    "$C_k = \\sum_i (a_i^{(1)} - y_i)^2$ <br>\n",
    "\n",
    "The training data becomes a vector too.\n",
    "\n",
    "$x -> \\mathbf x$ and has the same number of elements as input neurons. <br>\n",
    "$y -> \\mathbf y$ and has the same number of elements as output neurons. <br>\n",
    "\n",
    "This allows us to write the cost function in vector form using the modulus squared.<br>\n",
    "$C_k = |\\mathbf a^{(1)} - \\mathbf y|$ \n",
    "\n",
    "### Backpropagation\n",
    "Training this network is done by back-propagation because we start at the output layer and calculate derivatives backwards towards the input layer with the chain rule. To calculate the derivative of the cost with respect to the weight or bias (generalized any layer):\n",
    "\n",
    "$\n",
    "\\frac{\\partial C_k}{\\partial \\mathbf W^{(i)}} = \\frac{\\partial C_k}{\\partial \\mathbf a^{(N)}} \\frac{\\partial \\mathbf a^{(N)}}{\\partial \\mathbf a^{(N-1)}} ... \\frac{\\partial \\mathbf a^{(i+1)}}{\\partial \\mathbf a^{(i)}} \\frac{\\partial \\mathbf a^{(i)}}{\\partial \\mathbf z^{(i)}} \\frac{\\partial \\mathbf z^{(i)}}{\\partial \\mathbf W^{(i)}}$<br>\n",
    "\n",
    "#### Extra Backpropagation\n",
    "In the next cells, you will be asked to complete functions for the Jacobian of the cost function with respect to the weights and biases.\n",
    "We will start with layer 3, which is the easiest, and work backwards through the layers.\n",
    "\n",
    "We'll define our Jacobians as,\n",
    "$$ \\mathbf{J}_{\\mathbf{W}^{(3)}} = \\frac{\\partial C}{\\partial \\mathbf{W}^{(3)}} $$\n",
    "$$ \\mathbf{J}_{\\mathbf{b}^{(3)}} = \\frac{\\partial C}{\\partial \\mathbf{b}^{(3)}} $$\n",
    "etc., where $C$ is the average cost function over the training set. i.e.,\n",
    "$$ C = \\frac{1}{N}\\sum_k C_k $$\n",
    "You calculated the following in the practice quizzes,\n",
    "$$ \\frac{\\partial C}{\\partial \\mathbf{W}^{(3)}} =\n",
    "   \\frac{\\partial C}{\\partial \\mathbf{a}^{(3)}}\n",
    "   \\frac{\\partial \\mathbf{a}^{(3)}}{\\partial \\mathbf{z}^{(3)}}\n",
    "   \\frac{\\partial \\mathbf{z}^{(3)}}{\\partial \\mathbf{W}^{(3)}}\n",
    "   ,$$\n",
    "for the weight, and similarly for the bias,\n",
    "$$ \\frac{\\partial C}{\\partial \\mathbf{b}^{(3)}} =\n",
    "   \\frac{\\partial C}{\\partial \\mathbf{a}^{(3)}}\n",
    "   \\frac{\\partial \\mathbf{a}^{(3)}}{\\partial \\mathbf{z}^{(3)}}\n",
    "   \\frac{\\partial \\mathbf{z}^{(3)}}{\\partial \\mathbf{b}^{(3)}}\n",
    "   .$$\n",
    "With the partial derivatives taking the form,\n",
    "$$ \\frac{\\partial C}{\\partial \\mathbf{a}^{(3)}} = 2(\\mathbf{a}^{(3)} - \\mathbf{y}) $$\n",
    "$$ \\frac{\\partial \\mathbf{a}^{(3)}}{\\partial \\mathbf{z}^{(3)}} = \\sigma'({z}^{(3)})$$\n",
    "$$ \\frac{\\partial \\mathbf{z}^{(3)}}{\\partial \\mathbf{W}^{(3)}} = \\mathbf{a}^{(2)}$$\n",
    "$$ \\frac{\\partial \\mathbf{z}^{(3)}}{\\partial \\mathbf{b}^{(3)}} = 1$$\n",
    "\n",
    "We'll do the J_W3 ($\\mathbf{J}_{\\mathbf{W}^{(3)}}$) function for you, so you can see how it works.\n",
    "You should then be able to adapt the J_b3 function, with help, yourself.\n",
    "\n",
    "We'll next do the Jacobian for the Layer 2. The partial derivatives for this are,\n",
    "$$ \\frac{\\partial C}{\\partial \\mathbf{W}^{(2)}} =\n",
    "   \\frac{\\partial C}{\\partial \\mathbf{a}^{(3)}}\n",
    "   \\left(\n",
    "   \\frac{\\partial \\mathbf{a}^{(3)}}{\\partial \\mathbf{a}^{(2)}}\n",
    "   \\right)\n",
    "   \\frac{\\partial \\mathbf{a}^{(2)}}{\\partial \\mathbf{z}^{(2)}}\n",
    "   \\frac{\\partial \\mathbf{z}^{(2)}}{\\partial \\mathbf{W}^{(2)}}\n",
    "   ,$$\n",
    "$$ \\frac{\\partial C}{\\partial \\mathbf{b}^{(2)}} =\n",
    "   \\frac{\\partial C}{\\partial \\mathbf{a}^{(3)}}\n",
    "   \\left(\n",
    "   \\frac{\\partial \\mathbf{a}^{(3)}}{\\partial \\mathbf{a}^{(2)}}\n",
    "   \\right)\n",
    "   \\frac{\\partial \\mathbf{a}^{(2)}}{\\partial \\mathbf{z}^{(2)}}\n",
    "   \\frac{\\partial \\mathbf{z}^{(2)}}{\\partial \\mathbf{b}^{(2)}}\n",
    "   .$$\n",
    "This is very similar to the previous layer, with two exceptions:\n",
    "* There is a new partial derivative, in parentheses, $\\frac{\\partial \\mathbf{a}^{(3)}}{\\partial \\mathbf{a}^{(2)}}$\n",
    "* The terms after the parentheses are now one layer lower.\n",
    "\n",
    "Recall the new partial derivative takes the following form,\n",
    "$$ \\frac{\\partial \\mathbf{a}^{(3)}}{\\partial \\mathbf{a}^{(2)}} =\n",
    "   \\frac{\\partial \\mathbf{a}^{(3)}}{\\partial \\mathbf{z}^{(3)}}\n",
    "   \\frac{\\partial \\mathbf{z}^{(3)}}{\\partial \\mathbf{a}^{(2)}} =\n",
    "   \\sigma'(\\mathbf{z}^{(3)})\n",
    "   \\mathbf{W}^{(3)}\n",
    "$$\n",
    "\n",
    "To show how this changes things, we will implement the Jacobian for the weight again and ask you to implement it for the bias.\n",
    "\n",
    "Layer 1 is very similar to Layer 2, but with an addition partial derivative term.\n",
    "$$ \\frac{\\partial C}{\\partial \\mathbf{W}^{(1)}} =\n",
    "   \\frac{\\partial C}{\\partial \\mathbf{a}^{(3)}}\n",
    "   \\left(\n",
    "   \\frac{\\partial \\mathbf{a}^{(3)}}{\\partial \\mathbf{a}^{(2)}}\n",
    "   \\frac{\\partial \\mathbf{a}^{(2)}}{\\partial \\mathbf{a}^{(1)}}\n",
    "   \\right)\n",
    "   \\frac{\\partial \\mathbf{a}^{(1)}}{\\partial \\mathbf{z}^{(1)}}\n",
    "   \\frac{\\partial \\mathbf{z}^{(1)}}{\\partial \\mathbf{W}^{(1)}}\n",
    "   ,$$\n",
    "$$ \\frac{\\partial C}{\\partial \\mathbf{b}^{(1)}} =\n",
    "   \\frac{\\partial C}{\\partial \\mathbf{a}^{(3)}}\n",
    "   \\left(\n",
    "   \\frac{\\partial \\mathbf{a}^{(3)}}{\\partial \\mathbf{a}^{(2)}}\n",
    "   \\frac{\\partial \\mathbf{a}^{(2)}}{\\partial \\mathbf{a}^{(1)}}\n",
    "   \\right)\n",
    "   \\frac{\\partial \\mathbf{a}^{(1)}}{\\partial \\mathbf{z}^{(1)}}\n",
    "   \\frac{\\partial \\mathbf{z}^{(1)}}{\\partial \\mathbf{b}^{(1)}}\n",
    "   .$$\n",
    "You should be able to adapt lines from the previous cells to complete **both** the weight and bias Jacobian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taylor series and linearisation\n",
    "it's worth stopping for a minute to talk about when it might be useful to have an approximation. we're going to work through the derivation of Power Series representation of functions, which in short is the idea that you can use a series of increasing powers of x to re-express functions. So, if I know everything about it at one place, I also know everything about it everywhere. However, this is only true for a certain type of function that we call well __behaved__, which means **functions that are continuous and that you can differentiate as many times as you want**. \n",
    "\n",
    "$g_0(x) = f(0)$ <br>\n",
    "$g_1(x) = f'(0)x + f(0)$ <br>\n",
    "$g_2(x) = \\frac{1}{2}f''(0)x^2 + f'(0)x + f(0)$<br>\n",
    "$g_3(x) = \\frac{1}{6}f^{(3)}(0)x^3 + \\frac{1}{2}f''(0)x^2 + f'(0)x + f(0)$<br>\n",
    "$g(x) = \\sum_n^{\\infty} \\frac{1}{n!}f^{(n)}(0)x^n$ (Colin Maclaurin Series 1698 - 1746)\n",
    "\n",
    "We're going to be applying the concept of power series to some interesting cases as well as generalizing it to higher dimensions where rather than building approximation curves, we will be constructing approximation hyper surfaces. But whereas the Maclaurin series says that if you know everything about a function at the point x equals zero, then you can reconstruct everything about it everywhere. The Taylor series simply acknowledges that there is nothing special about the point x equals zero. And so says that if you know everything about the function at any point, then you can reconstruct the function anywhere.\n",
    "\n",
    "$g_0(x) = f(p)$ <br>\n",
    "$g_1(x) = f(p) + f'(p)(x-p)$ <br>\n",
    "$g_2(x) = f(p) + f'(p)(x-p) + \\frac{1}{2}f''(p)(x-p)^2$<br>\n",
    "$g(x) = \\sum_n^{\\infty} \\frac{1}{n!}f^{(n)}(p)(x-p)^n$ (Brook Taylor Series 1685 - 1731)\n",
    "\n",
    "Taylor series works with well behaved functions but poorly with bad behaved functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linearisation\n",
    "Run x Gradient = Rise (remember)<br>\n",
    "$(x-p) f'(p) = Rise$ <br>\n",
    "\n",
    "So we can now rewrite our first order approximation to include an error term, which we just say is on the order of delta x squared, or equally that it is second order accurate.\n",
    "\n",
    "$f(x + \\Delta x) = f(x) + f'(x)(\\Delta x) + O(\\Delta x^2)$<br>\n",
    "\n",
    "If we notice that the first of the higher order terms has a delta x in it. We know that we can now lump them all together and say that using the rise of a run method between two points with a finite separation will give us an approximation to the gradient that contains an error proportional to delta x. Or more simply put, the forward difference method is first order accurate.\n",
    "\n",
    "$f'(x) = \\frac{f(x + \\Delta x) - f(x)}{\\Delta x} + O(\\Delta x)$\n",
    "\n",
    "This process of taking a function and ignoring the terms above delta x is referred as linearisation and I hope it's now clear to you why that's the case.\n",
    "\n",
    "$f(x + \\Delta x) = \\sum_n^{\\infty} \\frac{1}{n!}f^{(n)}(x)\\Delta x^n$ <br>\n",
    "$f(x + \\Delta x) = f(x) + f'(x)\\Delta x + \\frac{1}{2} f''(x)\\Delta x^2 + \\frac{1}{6} f^{(3)}(x) \\Delta x^3 + ...$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate Taylor\n",
    "$f(x + \\Delta x, y + \\Delta y) = f(x,y) + (\\partial_x f(x,y) \\Delta x + \\partial_y f(x,y) \\Delta y) + \\frac{1}{2}(\\partial_{xx}f(x,y) \\Delta x^2 + 2 \\partial_{xy} f(x,y) \\Delta x \\Delta y + \\partial_{yy}f(x,y) \\Delta y^2)+ ...$\n",
    "\n",
    "So we now have a nice compact expression for the second order multivariate Taylor series expansion, which brings together so much of our calculus and linear algebra skills, and makes use of the Jacobian and Hessian concepts which we defined earlier in the course:\n",
    "\n",
    "$f(\\mathbf x + \\mathbf \\Delta x) = f(\\mathbf x) + J_f \\mathbf \\Delta x + \\frac{1}{2}\\mathbf \\Delta x^T H_f \\mathbf \\Delta x + ... $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
